<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Arraymancer - Module nnp_convolution</title>

  <link href="docutils.css" rel="stylesheet" type="text/css"/>
  <link href="nav.css" rel="stylesheet" type="text/css"/>

  <link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
  <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>
</head>
<body>
<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace">
      <span>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="tensor.accessors_macros_read.html">tensor.accessors_macros_read</a></li>
          <li><a href="tensor.accessors_macros_syntax.html">tensor.accessors_macros_syntax</a></li>
          <li><a href="tensor.accessors_macros_write.html">tensor.accessors_macros_write</a></li>
          <li><a href="tensor.accessors.html">tensor.accessors</a></li>
          <li><a href="tensor.aggregate.html">tensor.aggregate</a></li>
          <li><a href="tensor.comparison.html">tensor.comparison</a></li>
          <li><a href="tensor.data_structure.html">tensor.data_structure</a></li>
          <li><a href="tensor.display.html">tensor.display</a></li>
          <li><a href="tensor.exporting.html">tensor.exporting</a></li>
          <li><a href="tensor.filling_data.html">tensor.filling_data</a></li>
          <li><a href="tensor.higher_order_applymap.html">tensor.higher_order_applymap</a></li>
          <li><a href="tensor.higher_order_foldreduce.html">tensor.higher_order_foldreduce</a></li>
          <li><a href="tensor.init_cpu.html">tensor.init_cpu</a></li>
          <li><a href="tensor.init_copy_cpu.html">tensor.init_copy_cpu</a></li>
          <li><a href="tensor.lapack.html">tensor.lapack</a></li>
          <li><a href="tensor.math_functions.html">tensor.math_functions</a></li>
          <li><a href="tensor.operators_blas_l1.html">tensor.operators_blas_l1</a></li>
          <li><a href="tensor.operators_blas_l2l3.html">tensor.operators_blas_l2l3</a></li>
          <li><a href="tensor.operators_broadcasted.html">tensor.operators_broadcasted</a></li>
          <li><a href="tensor.operators_logical.html">tensor.operators_logical</a></li>
          <li><a href="tensor.optim_ops_fusion.html">tensor.optim_ops_fusion</a></li>
          <li><a href="tensor.shapeshifting.html">tensor.shapeshifting</a></li>
          <li><a href="tensor.syntactic_sugar.html">tensor.syntactic_sugar</a></li>
          <li><a href="tensor.ufunc.html">tensor.ufunc</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="ag.ag_accessors.html">Accessors</a></li>
          <li><a href="ag.ag_data_structure.html">Data structure</a></li>
          <li><a href="ag.gates_basic.html">Basic operations</a></li>
          <li><a href="ag.gates_blas.html">Linear algebra operations</a></li>
          <li><a href="ag.gates_reduce.html">Reduction operations</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="nn_activation.relu.html">Activation: Relu (Rectified linear Unit)</a></li>
          <li><a href="nn_activation.sigmoid.html">Activation: Sigmoid</a></li>
          <li><a href="nn_layers.conv2D.html">Layers: Convolution</a></li>
          <li><a href="nn_layers.conv2D.html">Layers: Linear/Dense</a></li>
          <li><a href="nn_loss.sigmoid_cross_entropy.html">Loss: Sigmoid Cross-Entropy</a></li>
          <li><a href="nn_optimizers.optimizers.html">Optimizers</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="nnp.nnp_activation.html">Activation</a></li>
          <li><a href="nnp.nnp_convolution.html">Convolution</a></li>
          <li><a href="nnp.nnp_linear.html">Linear / Dense layer</a></li>
          <li><a href="nnp.nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
          <li><a href="nnp.nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
        </ul>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
    </ul>
  </span>
</header>
<article id="documentId">
  <div class="container">
    <h1 class="title">Module nnp_convolution</h1>
    <div class="row">
  <div class="three columns">
  <div id="global-links">
    <ul class="simple">
    </ul>
  </div>
  <div id="searchInput">
    Search: <input type="text" id="searchInput"
      onkeyup="search()" />
  </div>
  <div>
    Group by:
    <select onchange="groupBy(this.value)">
      <option value="section">Section</option>
      <option value="type">Type</option>
    </select>
  </div>
  <ul class="simple simple-toc" id="toc-list">
<li>
  <a class="reference reference-toplevel" href="#6" id="56">Imports</a>
  <ul class="simple simple-toc-section">
    
  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#7" id="57">Types</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#Conv2DAlgorithm"
    title="Conv2DAlgorithm = enum
  Im2ColGEMM, NNPackAuto"><wbr />Conv2DAlgorithm<span class="attachedType" style="visibility:hidden"></span></a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#12" id="62">Procs</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#conv2d,Tensor[T],Tensor[T],Tensor[T],Size2D,Size2D"
    title="conv2d[T](input, weight, bias: Tensor[T]; padding: Size2D = (0, 0);
          stride: Size2D = (1, 1); algorithm = Conv2DAlgorithm.Im2ColGEMM): Tensor[T]"><wbr />conv2d<span class="attachedType" style="visibility:hidden">Conv2DAlgorithm</span></a></li>
  <li><a class="reference" href="#conv2d_backward,Tensor[T],Tensor[T],Tensor[T],Size2D,Size2D,Tensor[T],Tensor[T],Tensor[T],Tensor[T]"
    title="conv2d_backward[T](input, weight, bias: Tensor[T]; padding: Size2D; stride: Size2D;
                   grad_output: Tensor[T];
                   grad_input, grad_weight, grad_bias: var Tensor[T];
                   algorithm = Conv2DAlgorithm.Im2ColGEMM)"><wbr />conv2d_<wbr />backward<span class="attachedType" style="visibility:hidden">Conv2DAlgorithm</span></a></li>

  </ul>
</li>

</ul>

  </div>
  <div class="nine columns" id="content">
  <div id="tocRoot"></div>
  <p class="module-desc"></p>
  <div class="section" id="6">
<h1><a class="toc-backref" href="#6">Imports</a></h1>
<dl class="item">
<a class="reference external" href="../tensor/tensor.html">../tensor/tensor</a>, <a class="reference external" href="./private/p_nnp_types.html">./private/p_nnp_types</a>, <a class="reference external" href="./fallback/conv.html">./fallback/conv</a>
</dl></div>
<div class="section" id="7">
<h1><a class="toc-backref" href="#7">Types</a></h1>
<dl class="item">
<dt id="Conv2DAlgorithm"><a name="Conv2DAlgorithm"></a><pre><span class="Identifier">Conv2DAlgorithm</span> <span class="Other">=</span> <span class="Keyword">enum</span>
  <span class="Identifier">Im2ColGEMM</span><span class="Other">,</span> <span class="Identifier">NNPackAuto</span></pre></dt>
<dd>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/nn_primitives/nnp_convolution.nim#L24"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn_primitives/nnp_convolution.nim#L24" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>
<div class="section" id="12">
<h1><a class="toc-backref" href="#12">Procs</a></h1>
<dl class="item">
<dt id="conv2d"><a name="conv2d,Tensor[T],Tensor[T],Tensor[T],Size2D,Size2D"></a><pre><span class="Keyword">proc</span> <span class="Identifier">conv2d</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">input</span><span class="Other">,</span> <span class="Identifier">weight</span><span class="Other">,</span> <span class="Identifier">bias</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">padding</span><span class="Other">:</span> <span class="Identifier">Size2D</span> <span class="Other">=</span> <span class="Other">(</span><span class="DecNumber">0</span><span class="Other">,</span> <span class="DecNumber">0</span><span class="Other">)</span><span class="Other">;</span>
              <span class="Identifier">stride</span><span class="Other">:</span> <span class="Identifier">Size2D</span> <span class="Other">=</span> <span class="Other">(</span><span class="DecNumber">1</span><span class="Other">,</span> <span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">algorithm</span> <span class="Other">=</span> <span class="Identifier">Conv2DAlgorithm</span><span class="Other">.</span><span class="Identifier">Im2ColGEMM</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other pragmabegin">{.</span><div class="pragma">
    <span class="Identifier">inline</span></div><span class="Other pragmaend">.}</span></pre></dt>
<dd>
Computes a 2D convolution over input images. Intended to be used in 2d convolution forward pass. This applies a 2D cross-correlation, not to be confused with the mathematical convolution.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li><tt class="docutils literal"><span class="pre">input</span></tt> 4D Tensor batch of images of the size [N,C_in,H_in,W_in]</li>
<li><tt class="docutils literal"><span class="pre">weight</span></tt> 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW]</li>
<li><tt class="docutils literal"><span class="pre">bias</span></tt> 3D Tensor bias of the size [C_out,1,1] or an empty tensor for no bias</li>
<li><tt class="docutils literal"><span class="pre">padding</span></tt> Size2D tuple with height and width of the padding</li>
<li><tt class="docutils literal"><span class="pre">stride</span></tt> Size2D tuple with height and width of the stride</li>
<li><tt class="docutils literal"><span class="pre">algorithm</span></tt> algorithm to be used in the convolution</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li><dl class="docutils"><dt>A 4D Tensor of sized [N,C_out,H_out,W_out], where</dt>
<dd>H_out = (H_in + (2*padding.height) - kH) / stride.height + 1 W_out = (W_in + (2*padding.width) - kW) / stride.width + 1</dd>
</dl>
</li>
</ul>
</dd>
<dt>Valid algorithms:</dt>
<dd><ul class="simple"><li><tt class="docutils literal"><span class="pre">Im2ColGEMM</span></tt> im2col + GEMM algorithm, this is the default</li>
<li><tt class="docutils literal"><span class="pre">NNPackAuto</span></tt> Use NNPack and let it auto detect the best algorithm</li>
</ul>
</dd>
<dt>Future:</dt>
<dd>bias will leverage the upcoming Optional type to be really optional.</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/nn_primitives/nnp_convolution.nim#L28"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn_primitives/nnp_convolution.nim#L28" class="link-seesrc" target="_blank" >Edit</a>

</dd>
<dt id="conv2d_backward"><a name="conv2d_backward,Tensor[T],Tensor[T],Tensor[T],Size2D,Size2D,Tensor[T],Tensor[T],Tensor[T],Tensor[T]"></a><pre><span class="Keyword">proc</span> <span class="Identifier">conv2d_backward</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">input</span><span class="Other">,</span> <span class="Identifier">weight</span><span class="Other">,</span> <span class="Identifier">bias</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">padding</span><span class="Other">:</span> <span class="Identifier">Size2D</span><span class="Other">;</span>
                       <span class="Identifier">stride</span><span class="Other">:</span> <span class="Identifier">Size2D</span><span class="Other">;</span> <span class="Identifier">grad_output</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                       <span class="Identifier">grad_input</span><span class="Other">,</span> <span class="Identifier">grad_weight</span><span class="Other">,</span> <span class="Identifier">grad_bias</span><span class="Other">:</span> <span class="Keyword">var</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                       <span class="Identifier">algorithm</span> <span class="Other">=</span> <span class="Identifier">Conv2DAlgorithm</span><span class="Other">.</span><span class="Identifier">Im2ColGEMM</span><span class="Other">)</span></pre></dt>
<dd>
Computes gradients of a 2D convolution. Intended to be used after <tt class="docutils literal"><span class="pre">conv2d</span></tt> to calculate gradients in backward pass.<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li><tt class="docutils literal"><span class="pre">input</span></tt> 4D Tensor batch of images of the size [N,C_in,H_in,W_in]</li>
<li><tt class="docutils literal"><span class="pre">weight</span></tt> 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW]</li>
<li><tt class="docutils literal"><span class="pre">bias</span></tt> 3D Tensor bias of the size [C_out,1,1] or an empty tensor for no bias</li>
<li><tt class="docutils literal"><span class="pre">padding</span></tt> Size2D tuple with height and width of the padding</li>
<li><tt class="docutils literal"><span class="pre">stride</span></tt> Size2D tuple with height and width of the stride</li>
<li><tt class="docutils literal"><span class="pre">grad_output</span></tt> 4D tensor gradient of the next layer of the size [N,C_out,H_out,W_out]</li>
<li><tt class="docutils literal"><span class="pre">grad_input</span></tt> tensor where the gradient w.r.t input will be written</li>
<li><tt class="docutils literal"><span class="pre">grad_weight</span></tt> tensor where the gradient w.r.t weight will be written</li>
<li><tt class="docutils literal"><span class="pre">grad_bias</span></tt> tensor where the gradient w.r.t bias will be written</li>
<li><tt class="docutils literal"><span class="pre">algorithm</span></tt> algorithm to be used in the convolution</li>
</ul>
</dd>
<dt>Valid algorithms:</dt>
<dd><ul class="simple"><li><tt class="docutils literal"><span class="pre">Im2ColGEMM</span></tt> im2col + GEMM algorithm, this is the default</li>
<li><tt class="docutils literal"><span class="pre">NNPackAuto</span></tt> Use NNPack and let it auto detect the best algorithm</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/mratsim/arraymancer/tree/master/src/nn_primitives/nnp_convolution.nim#L65"
class="link-seesrc" target="_blank">Source</a>
<a href="https://github.com/mratsim/arraymancer/edit/master/src/nn_primitives/nnp_convolution.nim#L65" class="link-seesrc" target="_blank" >Edit</a>

</dd>

</dl></div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small>Made with Nim. Generated: 2017-12-13 23:30:26 UTC</small>
      </div>
    </div>
  </div>
</article>
</body>
</html>
