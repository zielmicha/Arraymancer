<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Arraymancer - Spellbook: How to do a multilayer perceptron</title>

  <link href="docutils.css" rel="stylesheet" type="text/css"/>
  <link href="nav.css" rel="stylesheet" type="text/css"/>

  <link href='http://fonts.googleapis.com/css?family=Raleway:400,600,900' rel='stylesheet' type='text/css'/>
  <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>
</head>
<body>
<a href="https://github.com/mratsim/arraymancer"><img style="position: fixed; top: 0; right: 0; border: 0; z-index: 10;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<header>
  <a class="pagetitle" href="index.html">Arraymancer</a>
  <span>
    <a href="#">Technical reference</a>
    <ul class="monospace">
      <span>
        <a href="#">Core tensor API</a>
        <ul class="monospace">
          <li><a href="tensor.accessors_macros_read.html">tensor.accessors_macros_read</a></li>
          <li><a href="tensor.accessors_macros_syntax.html">tensor.accessors_macros_syntax</a></li>
          <li><a href="tensor.accessors_macros_write.html">tensor.accessors_macros_write</a></li>
          <li><a href="tensor.accessors.html">tensor.accessors</a></li>
          <li><a href="tensor.aggregate.html">tensor.aggregate</a></li>
          <li><a href="tensor.comparison.html">tensor.comparison</a></li>
          <li><a href="tensor.data_structure.html">tensor.data_structure</a></li>
          <li><a href="tensor.display.html">tensor.display</a></li>
          <li><a href="tensor.exporting.html">tensor.exporting</a></li>
          <li><a href="tensor.filling_data.html">tensor.filling_data</a></li>
          <li><a href="tensor.higher_order_applymap.html">tensor.higher_order_applymap</a></li>
          <li><a href="tensor.higher_order_foldreduce.html">tensor.higher_order_foldreduce</a></li>
          <li><a href="tensor.init_cpu.html">tensor.init_cpu</a></li>
          <li><a href="tensor.init_copy_cpu.html">tensor.init_copy_cpu</a></li>
          <li><a href="tensor.lapack.html">tensor.lapack</a></li>
          <li><a href="tensor.math_functions.html">tensor.math_functions</a></li>
          <li><a href="tensor.operators_blas_l1.html">tensor.operators_blas_l1</a></li>
          <li><a href="tensor.operators_blas_l2l3.html">tensor.operators_blas_l2l3</a></li>
          <li><a href="tensor.operators_broadcasted.html">tensor.operators_broadcasted</a></li>
          <li><a href="tensor.operators_logical.html">tensor.operators_logical</a></li>
          <li><a href="tensor.optim_ops_fusion.html">tensor.optim_ops_fusion</a></li>
          <li><a href="tensor.shapeshifting.html">tensor.shapeshifting</a></li>
          <li><a href="tensor.syntactic_sugar.html">tensor.syntactic_sugar</a></li>
          <li><a href="tensor.ufunc.html">tensor.ufunc</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Autograd</a>
        <ul class="monospace">
          <li><a href="ag.ag_accessors.html">Accessors</a></li>
          <li><a href="ag.ag_data_structure.html">Data structure</a></li>
          <li><a href="ag.gates_basic.html">Basic operations</a></li>
          <li><a href="ag.gates_blas.html">Linear algebra operations</a></li>
          <li><a href="ag.gates_reduce.html">Reduction operations</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neural network API</a>
        <ul class="monospace">
          <li><a href="nn_activation.relu.html">Activation: Relu (Rectified linear Unit)</a></li>
          <li><a href="nn_activation.sigmoid.html">Activation: Sigmoid</a></li>
          <li><a href="nn_layers.conv2D.html">Layers: Convolution</a></li>
          <li><a href="nn_layers.conv2D.html">Layers: Linear/Dense</a></li>
          <li><a href="nn_loss.sigmoid_cross_entropy.html">Loss: Sigmoid Cross-Entropy</a></li>
          <li><a href="nn_optimizers.optimizers.html">Optimizers</a></li>
        </ul>
      </span>
      <span>
        <a href="#">Neuralnet primitives</a>
        <ul class="monospace">
          <li><a href="nnp.nnp_activation.html">Activation</a></li>
          <li><a href="nnp.nnp_convolution.html">Convolution</a></li>
          <li><a href="nnp.nnp_linear.html">Linear / Dense layer</a></li>
          <li><a href="nnp.nnp_sigmoid_cross_entropy.html">Sigmoid Cross-Entropy loss</a></li>
          <li><a href="nnp.nnp_softmax_cross_entropy.html">Softmax Cross-Entropy loss</a></li>
        </ul>
      </span>
    </ul>
  </span>
  <span>
    <a href="#">Tutorial</a>
    <ul class="monospace">
      <li><a href="tuto.first_steps.html">First steps</a></li>
      <li><a href="tuto.slicing.html">Taking a slice of a tensor</a></li>
      <li><a href="tuto.linear_algebra.html">Matrix & vectors operations</a></li>
      <li><a href="tuto.broadcasting.html">Broadcasted operations</a></li>
      <li><a href="tuto.shapeshifting.html">Transposing, Reshaping, Permuting, Concatenating</a></li>
      <li><a href="tuto.map_reduce.html">Map & Reduce</a></li>
      <li><a href="tuto.iterators.html">Basic iterators</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Spellbook (How-To&apos;s)</a>
    <ul class="monospace">
      <li><a href="howto.type_conversion.html">How to convert a Tensor type?</a></li>
      <li><a href="howto.ufunc.html">How to create a new universal function?</a></li>
      <li><a href="howto.perceptron.html">How to create a multilayer perceptron?</a></li>
    </ul>
  </span>
  <span>
    <a href="#">Under the hood</a>
    <ul class="monospace">
      <li><a href="uth.speed.html">How Arraymancer achieves its speed?</a></li>
      <li><a href="uth.copy_semantics.html">Why does `=` share data by default aka reference semantics?</a></li>
    </ul>
  </span>
</header>
<article id="documentId">
  <div class="container">
    <h1 class="title">Spellbook: How to do a multilayer perceptron</h1>
    <pre class="listing"><span class="Keyword">import</span> <span class="Operator">../</span><span class="Identifier">src</span><span class="Operator">/</span><span class="Identifier">arraymancer</span>

<span class="Comment"># Example multilayer perceptron in Arraymancer.</span>

<span class="Comment"># We will use as examples the OR function similar to this article:</span>
<span class="Comment">#Â https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/</span>

<span class="Keyword">let</span> <span class="Identifier">ctx</span> <span class="Operator">=</span> <span class="Identifier">newContext</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float32</span><span class="Punctuation">]</span>

<span class="Keyword">let</span> <span class="Identifier">bsz</span> <span class="Operator">=</span> <span class="DecNumber">32</span> <span class="Comment">#batch size</span>

<span class="Comment"># We will create a tensor of size 3200 --&gt; 100 batch sizes of 32</span>
<span class="Comment"># We create it as int between [0, 2[ (2 excluded) and convert to bool</span>
<span class="Keyword">let</span> <span class="Identifier">x_train_bool</span> <span class="Operator">=</span> <span class="Identifier">randomTensor</span><span class="Punctuation">(</span><span class="Punctuation">[</span><span class="Identifier">bsz</span> <span class="Operator">*</span> <span class="DecNumber">100</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">]</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">astype</span><span class="Punctuation">(</span><span class="Identifier">bool</span><span class="Punctuation">)</span>

<span class="Comment"># Let's build or truth labels. We need to apply xor between the 2 columns of the tensors</span>
<span class="Keyword">let</span> <span class="Identifier">y_bool</span> <span class="Operator">=</span> <span class="Identifier">x_train_bool</span><span class="Punctuation">[</span><span class="Identifier">_</span><span class="Punctuation">,</span><span class="DecNumber">0</span><span class="Punctuation">]</span> <span class="Keyword">xor</span> <span class="Identifier">x_train_bool</span><span class="Punctuation">[</span><span class="Identifier">_</span><span class="Punctuation">,</span><span class="DecNumber">1</span><span class="Punctuation">]</span>

<span class="Comment"># Convert to float and transpose so batch_size is last</span>
<span class="Keyword">let</span> <span class="Identifier">x_train</span> <span class="Operator">=</span> <span class="Identifier">ctx</span><span class="Operator">.</span><span class="Identifier">variable</span><span class="Punctuation">(</span><span class="Identifier">x_train_bool</span><span class="Operator">.</span><span class="Identifier">astype</span><span class="Punctuation">(</span><span class="Identifier">float32</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">transpose</span><span class="Punctuation">)</span>
<span class="Keyword">let</span> <span class="Identifier">y</span> <span class="Operator">=</span> <span class="Identifier">y_bool</span><span class="Operator">.</span><span class="Identifier">astype</span><span class="Punctuation">(</span><span class="Identifier">float32</span><span class="Punctuation">)</span><span class="Operator">.</span><span class="Identifier">transpose</span>

<span class="Comment"># First hidden layer of 3 neurons, with 2 features in</span>
<span class="Comment"># We initialize with random weights between -1 and 1</span>
<span class="Keyword">let</span> <span class="Identifier">layer_3neurons</span> <span class="Operator">=</span> <span class="Identifier">ctx</span><span class="Operator">.</span><span class="Identifier">variable</span><span class="Punctuation">(</span>
                      <span class="Identifier">randomTensor</span><span class="Punctuation">(</span><span class="DecNumber">3</span><span class="Punctuation">,</span> <span class="DecNumber">2</span><span class="Punctuation">,</span> <span class="FloatNumber">2.0</span><span class="Identifier">f</span><span class="Punctuation">)</span> <span class="Operator">.-</span> <span class="FloatNumber">1.0</span><span class="Identifier">f</span>
                      <span class="Punctuation">)</span>

<span class="Comment"># Classifier layer with 1 neuron per feature. (In our case only one neuron overall)</span>
<span class="Comment"># We initialize with random weights between -1 and 1</span>
<span class="Keyword">let</span> <span class="Identifier">classifier_layer</span> <span class="Operator">=</span> <span class="Identifier">ctx</span><span class="Operator">.</span><span class="Identifier">variable</span><span class="Punctuation">(</span>
                  <span class="Identifier">randomTensor</span><span class="Punctuation">(</span><span class="DecNumber">1</span><span class="Punctuation">,</span> <span class="DecNumber">3</span><span class="Punctuation">,</span> <span class="FloatNumber">2.0</span><span class="Identifier">f</span><span class="Punctuation">)</span> <span class="Operator">.-</span> <span class="FloatNumber">1.0</span><span class="Identifier">f</span>
                  <span class="Punctuation">)</span>

<span class="Comment"># Stochastic Gradient Descent</span>
<span class="Keyword">let</span> <span class="Identifier">optim</span> <span class="Operator">=</span> <span class="Identifier">newSGD</span><span class="Punctuation">[</span><span class="Identifier">float32</span><span class="Punctuation">]</span><span class="Punctuation">(</span>
  <span class="Identifier">layer_3neurons</span><span class="Punctuation">,</span> <span class="Identifier">classifier_layer</span><span class="Punctuation">,</span> <span class="FloatNumber">0.01</span><span class="Identifier">f</span> <span class="Comment"># 0.01 is the learning rate</span>
<span class="Punctuation">)</span>

<span class="Keyword">for</span> <span class="Identifier">epoch</span> <span class="Keyword">in</span> <span class="FloatNumber">0.</span><span class="Operator">.</span><span class="DecNumber">10000</span><span class="Punctuation">:</span>
  
  <span class="Keyword">for</span> <span class="Identifier">batch_id</span> <span class="Keyword">in</span> <span class="FloatNumber">0.</span><span class="Operator">.&lt;</span><span class="DecNumber">100</span><span class="Punctuation">:</span>
    
    <span class="Comment"># offset in the Tensor (Remember, batch size is last)</span>
    <span class="Keyword">let</span> <span class="Identifier">offset</span> <span class="Operator">=</span> <span class="Identifier">batch_id</span> <span class="Operator">*</span> <span class="DecNumber">32</span>
    <span class="Keyword">let</span> <span class="Identifier">x</span> <span class="Operator">=</span> <span class="Identifier">x_train</span><span class="Punctuation">[</span><span class="Identifier">_</span><span class="Punctuation">,</span> <span class="Identifier">offset</span> <span class="Operator">..&lt;</span> <span class="Identifier">offset</span> <span class="Operator">+</span> <span class="DecNumber">32</span><span class="Punctuation">]</span>
    <span class="Keyword">let</span> <span class="Identifier">target</span> <span class="Operator">=</span> <span class="Identifier">y</span><span class="Punctuation">[</span><span class="Identifier">_</span><span class="Punctuation">,</span> <span class="Identifier">offset</span> <span class="Operator">..&lt;</span> <span class="Identifier">offset</span> <span class="Operator">+</span> <span class="DecNumber">32</span><span class="Punctuation">]</span>
    
    <span class="Comment"># Building the network</span>
    <span class="Keyword">let</span> <span class="Identifier">n1</span> <span class="Operator">=</span> <span class="Identifier">linear</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">,</span> <span class="Identifier">layer_3neurons</span><span class="Punctuation">)</span>
    <span class="Keyword">let</span> <span class="Identifier">n1_relu</span> <span class="Operator">=</span> <span class="Identifier">n1</span><span class="Operator">.</span><span class="Identifier">relu</span>
    <span class="Keyword">let</span> <span class="Identifier">n2</span> <span class="Operator">=</span> <span class="Identifier">linear</span><span class="Punctuation">(</span><span class="Identifier">n1_relu</span><span class="Punctuation">,</span> <span class="Identifier">classifier_layer</span><span class="Punctuation">)</span>
    <span class="Keyword">let</span> <span class="Identifier">loss</span> <span class="Operator">=</span> <span class="Identifier">sigmoid_cross_entropy</span><span class="Punctuation">(</span><span class="Identifier">n2</span><span class="Punctuation">,</span> <span class="Identifier">target</span><span class="Punctuation">)</span>
    
    <span class="Comment"># Compute the gradient (i.e. contribution of each parameter to the loss)</span>
    <span class="Identifier">loss</span><span class="Operator">.</span><span class="Identifier">backprop</span><span class="Punctuation">(</span><span class="Punctuation">)</span>
    
    <span class="Comment"># Correct the weights now that we have the gradient information</span>
    <span class="Identifier">optim</span><span class="Operator">.</span><span class="Identifier">update</span><span class="Punctuation">(</span><span class="Punctuation">)</span></pre>


    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small>Made with Nim. Generated: 2017-12-13 23:30:37 UTC</small>
      </div>
    </div>
  </div>
</article>
</body>
</html>
